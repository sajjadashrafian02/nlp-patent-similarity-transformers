{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(r\"F:\\programming\\fatsai\\Google Patent Phrase Similarity Dataset\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation=pd.read_csv(r\"F:\\programming\\fatsai\\Google Patent Phrase Similarity Dataset\\validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(r\"F:\\programming\\fatsai\\Google Patent Phrase Similarity Dataset\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['input'] = 'TEXT1: ' + train.context + '; TEXT2: ' + train.target + '; ANC1: ' + train.anchor\n",
    "validation['input'] = 'TEXT1: ' + validation.context + '; TEXT2: ' + validation.target + '; ANC1: ' + validation.anchor\n",
    "test['input'] = 'TEXT1: ' + test.context + '; TEXT2: ' + test.target + '; ANC1: ' + test.anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     TEXT1: C09; TEXT2: cholesterol; ANC1: aralkynyl\n",
       "1         TEXT1: C09; TEXT2: aralkyl; ANC1: aralkynyl\n",
       "2     TEXT1: C09; TEXT2: heterocycle; ANC1: aralkynyl\n",
       "3            TEXT1: C09; TEXT2: acyl; ANC1: aralkynyl\n",
       "4    TEXT1: C09; TEXT2: heterocyclic; ANC1: aralkynyl\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset #using datasel library (hugging face) to run model\n",
    "\n",
    "ds1 = Dataset.from_pandas(train) #cinverting our data from into Dataset\n",
    "ds2 = Dataset.from_pandas(validation)\n",
    "ds3 = Dataset.from_pandas(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small' #choosing our model(we need it here because every model has its own tokenazition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sajjad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sajjad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm) #our special tokenazier for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁hello', '▁guys', ',', '▁im', '▁saj', 'jad', '▁as', 'hra', 'fin', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.tokenize(\"hello guys, im sajjad ashrafin!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁the', '▁weather', '▁is', '▁so', '▁hot', '▁!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.tokenize(\"the weather is so hot !\") #Uncommon words  splitted  into pieces. The start of a new word is represented by ▁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tok_func at 0x0000024DCED95940> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cd4ba02e3749269b45c96ce5328868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4695552a60354de2a2256ef92e6ef9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096b11adc33d4d13a023741f2a72d463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tok_func(x): #a function that back tokenizied inpots\n",
    "    return tokz(x[\"input\"])\n",
    "tok_ds1 = ds1.map(tok_func, batched=True)# a mappinfg for doing parallel running that adds a new item to our dataset called input_ids\n",
    "tok_ds2 = ds2.map(tok_func, batched=True)\n",
    "tok_ds3 = ds3.map(tok_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT1: C09; TEXT2: cholesterol; ANC1: aralkynyl [1, 54453, 435, 294, 716, 4505, 346, 54453, 445, 294, 9888, 346, 23702, 435, 294, 266, 17226, 9593, 63791, 2]\n"
     ]
    }
   ],
   "source": [
    "row = tok_ds1[0]\n",
    "print(row['input'], row['input_ids'])#now we can convert every english covab in our data to numbers using new coloumn\n",
    "tok_ds1 = tok_ds1.rename_columns({'score':'labels'})\n",
    "tok_ds2 = tok_ds2.rename_columns({'score':'labels'})\n",
    "tok_ds3 = tok_ds3.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # we use correlation as our metric \n",
    "def corr(x,y): #we build corrlation using numpy\n",
    "    return np.corrcoef(x,y)[0][1] \n",
    "\n",
    "def corr_d(eval_pred): \n",
    "    return {'pearson': corr(*eval_pred)}# then convert it to the dictinery for transformers requirment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer #we need trainer to build model\n",
    "bs = 128 #our batch size\n",
    "epochs = 4 # and number of epochs \n",
    "lr = 8e-5 # starting learning rate\n",
    "args = TrainingArguments('outputs',#the directory where our wheight will save\n",
    "                        learning_rate=lr,#specifieng learning rate for start\n",
    "                        warmup_ratio=0.1,#how to increase learning with steps\n",
    "                        lr_scheduler_type='cosine', #lower the learning rate using a cosine curve \n",
    "                        fp16=True,\n",
    "                        evaluation_strategy=\"epoch\", #evaluate the model after each epochs \n",
    "                        per_device_train_batch_size=bs, #size of our batch \n",
    "                        per_device_eval_batch_size=bs*2, \n",
    "                        num_train_epochs=epochs,#total number of training epochs \n",
    "                        weight_decay=0.01,#usinh L2 regularzation\n",
    "                        report_to='none')# our argument for fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sajjad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #create our model\n",
    "trainer = Trainer(model, #specifieng model\n",
    "                args, #illustarate arguments\n",
    "                train_dataset=tok_ds1, #our train set\n",
    "                eval_dataset=tok_ds2, #our test set\n",
    "                tokenizer=tokz, #out tokenizer\n",
    "                compute_metrics=corr_d) #what is our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db28f5ea1994ffb83e794bd6a9c835d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91184f60a0bd47f3a914eb244e4e7a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02567780390381813, 'eval_pearson': 0.7861627477043353, 'eval_runtime': 26.5973, 'eval_samples_per_second': 106.89, 'eval_steps_per_second': 0.451, 'epoch': 1.0}\n",
      "{'loss': 0.034, 'grad_norm': 0.6738039255142212, 'learning_rate': 5.516578043170591e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d9b727051242ea95b2eb5553e19d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026195835322141647, 'eval_pearson': 0.7923502239694946, 'eval_runtime': 26.4853, 'eval_samples_per_second': 107.343, 'eval_steps_per_second': 0.453, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d32c7ee5ae444f89152aad1b01cf5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02681119740009308, 'eval_pearson': 0.7957460309786399, 'eval_runtime': 26.4809, 'eval_samples_per_second': 107.36, 'eval_steps_per_second': 0.453, 'epoch': 3.0}\n",
      "{'loss': 0.0143, 'grad_norm': 0.1502688080072403, 'learning_rate': 3.6193470242834415e-06, 'epoch': 3.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b9746a5dad43f6a24a89db5c57b30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02654223144054413, 'eval_pearson': 0.7952388016668152, 'eval_runtime': 26.4923, 'eval_samples_per_second': 107.314, 'eval_steps_per_second': 0.453, 'epoch': 4.0}\n",
      "{'train_runtime': 4875.4978, 'train_samples_per_second': 29.924, 'train_steps_per_second': 0.234, 'train_loss': 0.022605083072394654, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train(); #run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my_deberta_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-base' #choosing our model(we need it here because every model has its own tokenazition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm) #our special tokenazier for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁what', '▁a', '▁cool', '▁day', '▁is', '▁today', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.tokenize(\"what a cool day is today!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de316616c0496ea05d95133eca648d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d43a7ac85254ec3a6407f462e5034ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baddd9495c83404ca3a60b9517edaa1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tok_func(x): #a function that back tokenizied inpots\n",
    "    return tokz(x[\"input\"])\n",
    "tok_ds1 = ds1.map(tok_func, batched=True)# a mappinfg for doing parallel running that adds a new item to our dataset called input_ids\n",
    "tok_ds2 = ds2.map(tok_func, batched=True)\n",
    "tok_ds3 = ds3.map(tok_func, batched=True)\n",
    "tok_ds1 = tok_ds1.rename_columns({'score':'labels'})\n",
    "tok_ds2 = tok_ds2.rename_columns({'score':'labels'})\n",
    "tok_ds3 = tok_ds3.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8 #our batch size\n",
    "epochs = 2 # and number of epochs \n",
    "lr = 8e-5 # starting learning rate\n",
    "args = TrainingArguments('outputs',#the directory where our wheight will save\n",
    "                        learning_rate=lr,#specifieng learning rate for start\n",
    "                        warmup_ratio=0.1,#how to increase learning with steps\n",
    "                        lr_scheduler_type='cosine', #lower the learning rate using a cosine curve \n",
    "                        fp16=True,\n",
    "                        evaluation_strategy=\"epoch\", #evaluate the model after each epochs \n",
    "                        per_device_train_batch_size=bs, #size of our batch \n",
    "                        per_device_eval_batch_size=bs, \n",
    "                        gradient_accumulation_steps=2,  \n",
    "                        num_train_epochs=epochs,#total number of training epochs \n",
    "                        weight_decay=0.01,#usinh L2 regularzation\n",
    "                        report_to='none')# our argument for fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run  and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #create our model\n",
    "trainer = Trainer(model, #specifieng model\n",
    "                args, #illustarate arguments\n",
    "                train_dataset=tok_ds1, #our train set\n",
    "                eval_dataset=tok_ds2, #our test set\n",
    "                tokenizer=tokz, #out tokenizer\n",
    "                compute_metrics=corr_d) #what is our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706885a910a145f6b5c87be54db59898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0161, 'grad_norm': 0.7374388575553894, 'learning_rate': 7.997731288189876e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0187, 'grad_norm': 0.2470538467168808, 'learning_rate': 7.658156206974005e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0172, 'grad_norm': 0.34760668873786926, 'learning_rate': 6.789187204375981e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0156, 'grad_norm': 0.3444696366786957, 'learning_rate': 5.516578043170591e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2c48a128c54ed5bc42a099caf38d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029392264783382416, 'eval_pearson': 0.7609570315423655, 'eval_runtime': 72.004, 'eval_samples_per_second': 39.484, 'eval_steps_per_second': 4.944, 'epoch': 1.0}\n",
      "{'loss': 0.0132, 'grad_norm': 0.27112889289855957, 'learning_rate': 4.0244956960401305e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0103, 'grad_norm': 0.29238006472587585, 'learning_rate': 2.5288684285044283e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0112, 'grad_norm': 0.44043195247650146, 'learning_rate': 1.2461375129665934e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0116, 'grad_norm': 0.1736825406551361, 'learning_rate': 3.6193470242834415e-06, 'epoch': 1.75}\n",
      "{'loss': 0.012, 'grad_norm': 0.21496210992336273, 'learning_rate': 4.3600933994007996e-08, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a358147054ab48b6a15388f3b7e50253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026234453544020653, 'eval_pearson': 0.7914985209786293, 'eval_runtime': 71.9804, 'eval_samples_per_second': 39.497, 'eval_steps_per_second': 4.946, 'epoch': 2.0}\n",
      "{'train_runtime': 6788.875, 'train_samples_per_second': 10.745, 'train_steps_per_second': 0.672, 'train_loss': 0.013981913722920836, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(); #run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = \"nreimers/MiniLM-L6-H384-uncased\" #choosing our model(we need it here because every model has its own tokenazition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm) #our special tokenazier for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lets', 'explore', 'this', 'new', 'model', '!']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.tokenize(\"lets explore this new model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901e9bbe6ca04b5d853ff558d2e70a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805c75281d0f4d8da9d736713112aa91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c59fdd1afe4d7f8c3b102d43ee2bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tok_func(x): #a function that back tokenizied inpots\n",
    "    return tokz(x[\"input\"])\n",
    "tok_ds1 = ds1.map(tok_func, batched=True)# a mappinfg for doing parallel running that adds a new item to our dataset called input_ids\n",
    "tok_ds2 = ds2.map(tok_func, batched=True)\n",
    "tok_ds3 = ds3.map(tok_func, batched=True)\n",
    "tok_ds1 = tok_ds1.rename_columns({'score':'labels'})\n",
    "tok_ds2 = tok_ds2.rename_columns({'score':'labels'})\n",
    "tok_ds3 = tok_ds3.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def corr(x, y):\n",
    "    return np.corrcoef(x, y)[0][1]\n",
    "\n",
    "def corr_d(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Flatten the predictions if needed\n",
    "    if predictions.ndim > 1 and predictions.shape[1] == 1:\n",
    "        predictions = predictions.squeeze(axis=1)\n",
    "\n",
    "    return {'pearson': corr(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "bs = 16 #our batch size\n",
    "epochs = 8 # and number of epochs \n",
    "lr = 8e-5 # starting learning rate\n",
    "args = TrainingArguments('outputs',#the directory where our wheight will save\n",
    "                        learning_rate=lr,#specifieng learning rate for start\n",
    "                        warmup_ratio=0.1,#how to increase learning with steps\n",
    "                        lr_scheduler_type='cosine', #lower the learning rate using a cosine curve \n",
    "                        fp16=True,\n",
    "                        load_best_model_at_end=True,\n",
    "                        evaluation_strategy=\"epoch\", #evaluate the model after each epochs \n",
    "                        save_strategy=\"epoch\",        \n",
    "                        metric_for_best_model=\"pearson\",  \n",
    "                        per_device_train_batch_size=bs, #size of our batch \n",
    "                        per_device_eval_batch_size=bs*2, \n",
    "                        gradient_accumulation_steps=2,  \n",
    "                        num_train_epochs=epochs,#total number of training epochs \n",
    "                        weight_decay=0.01,#usinh L2 regularzation\n",
    "                        report_to='none')# our argument for fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #create our model\n",
    "trainer = Trainer(model, #specifieng model\n",
    "                args, #illustarate arguments\n",
    "                train_dataset=tok_ds1, #our train set\n",
    "                eval_dataset=tok_ds2, #our test set\n",
    "                tokenizer=tokz, #out tokenizer\n",
    "                compute_metrics=corr_d, #what is our metrics\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stops after 2 evals without improvement\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989d1cff222e40e9bebe1fe724def172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0132, 'grad_norm': 0.3463342487812042, 'learning_rate': 4.385964912280702e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0144, 'grad_norm': 0.2563301622867584, 'learning_rate': 7.997731288189876e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7449abd585d42fbbf40f7783fb3d4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03520682826638222, 'eval_pearson': 0.7109768359911046, 'eval_runtime': 6.0016, 'eval_samples_per_second': 473.71, 'eval_steps_per_second': 14.829, 'epoch': 1.0}\n",
      "{'loss': 0.0133, 'grad_norm': 0.24565860629081726, 'learning_rate': 7.899126790831869e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0134, 'grad_norm': 0.4267203211784363, 'learning_rate': 7.658156206974005e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2e9715ba184d18b83022dede047304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.035955775529146194, 'eval_pearson': 0.7019910588983029, 'eval_runtime': 5.9874, 'eval_samples_per_second': 474.829, 'eval_steps_per_second': 14.865, 'epoch': 2.0}\n",
      "{'loss': 0.0122, 'grad_norm': 0.1615705043077469, 'learning_rate': 7.283617926826647e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0109, 'grad_norm': 0.1913604587316513, 'learning_rate': 6.789187204375981e-05, 'epoch': 2.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad55adc30bbc411d9d131cb8111299d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03614410012960434, 'eval_pearson': 0.7116770502189527, 'eval_runtime': 6.1097, 'eval_samples_per_second': 465.328, 'eval_steps_per_second': 14.567, 'epoch': 3.0}\n",
      "{'loss': 0.0103, 'grad_norm': 0.3467991352081299, 'learning_rate': 6.192916842410089e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0086, 'grad_norm': 0.30314546823501587, 'learning_rate': 5.516578043170591e-05, 'epoch': 3.51}\n",
      "{'loss': 0.0089, 'grad_norm': 0.2180846929550171, 'learning_rate': 4.7848654916885446e-05, 'epoch': 3.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de58effb1f7d4a848c810ba39f7e5305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03363389894366264, 'eval_pearson': 0.7258992098634983, 'eval_runtime': 5.9542, 'eval_samples_per_second': 477.475, 'eval_steps_per_second': 14.947, 'epoch': 4.0}\n",
      "{'loss': 0.0078, 'grad_norm': 0.24860285222530365, 'learning_rate': 4.0244956960401305e-05, 'epoch': 4.39}\n",
      "{'loss': 0.0075, 'grad_norm': 0.1753646433353424, 'learning_rate': 3.2632315061931184e-05, 'epoch': 4.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff61da5ae3834b1ca341f338a1f09682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03488587960600853, 'eval_pearson': 0.7216647231086074, 'eval_runtime': 5.9354, 'eval_samples_per_second': 478.989, 'eval_steps_per_second': 14.995, 'epoch': 5.0}\n",
      "{'loss': 0.0081, 'grad_norm': 0.4730294346809387, 'learning_rate': 2.5288684285044283e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0081, 'grad_norm': 0.25007617473602295, 'learning_rate': 1.8482197478578303e-05, 'epoch': 5.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3b40aba5f84445bf58da9083010223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03555789589881897, 'eval_pearson': 0.7221306853107899, 'eval_runtime': 5.9537, 'eval_samples_per_second': 477.521, 'eval_steps_per_second': 14.949, 'epoch': 6.0}\n",
      "{'train_runtime': 1607.5468, 'train_samples_per_second': 181.509, 'train_steps_per_second': 5.673, 'train_loss': 0.010390488062685693, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(); #run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"nreimers/MiniLM-L6-H384-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm) #our special tokenazier for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'Ġa', 'Ġlong', 'Ġwork']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.tokenize(\"what a long work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb971140c3c42cba04d3b00dee9bae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac206e74a4654ade84fb6e42fb0bacf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9269447291584e539c839bb6fed6e865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tok_func(x): #a function that back tokenizied inpots\n",
    "    return tokz(x[\"input\"])\n",
    "tok_ds1 = ds1.map(tok_func, batched=True)# a mappinfg for doing parallel running that adds a new item to our dataset called input_ids\n",
    "tok_ds2 = ds2.map(tok_func, batched=True)\n",
    "tok_ds3 = ds3.map(tok_func, batched=True)\n",
    "tok_ds1 = tok_ds1.rename_columns({'score':'labels'})\n",
    "tok_ds2 = tok_ds2.rename_columns({'score':'labels'})\n",
    "tok_ds3 = tok_ds3.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8 #our batch size\n",
    "epochs = 2 # and number of epochs \n",
    "lr = 8e-5 # starting learning rate\n",
    "args = TrainingArguments('outputs',#the directory where our wheight will save\n",
    "                        learning_rate=lr,#specifieng learning rate for start\n",
    "                        warmup_ratio=0.1,#how to increase learning with steps\n",
    "                        lr_scheduler_type='cosine', #lower the learning rate using a cosine curve \n",
    "                        fp16=True,\n",
    "                        evaluation_strategy=\"epoch\", #evaluate the model after each epochs \n",
    "                        per_device_train_batch_size=bs, #size of our batch \n",
    "                        per_device_eval_batch_size=bs, \n",
    "                        gradient_accumulation_steps=2,  \n",
    "                        num_train_epochs=epochs,#total number of training epochs \n",
    "                        weight_decay=0.01,#usinh L2 regularzation\n",
    "                        report_to='none')# our argument for fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #create our model\n",
    "trainer = Trainer(model, #specifieng model\n",
    "                args, #illustarate arguments\n",
    "                train_dataset=tok_ds1, #our train set\n",
    "                eval_dataset=tok_ds2, #our test set\n",
    "                tokenizer=tokz, #out tokenizer\n",
    "                compute_metrics=corr_d) #what is our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6998d1850a4d42b9a0dc2e97367340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0287, 'grad_norm': 0.9536291360855103, 'learning_rate': 7.997731288189876e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0337, 'grad_norm': 0.6044930815696716, 'learning_rate': 7.658156206974005e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0315, 'grad_norm': 0.47513195872306824, 'learning_rate': 6.789187204375981e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0282, 'grad_norm': 0.7719506025314331, 'learning_rate': 5.519410964445069e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c7f1fde9fe47d4b8917b53ef0d05d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03383300080895424, 'eval_pearson': 0.7201047753800074, 'eval_runtime': 44.1889, 'eval_samples_per_second': 64.337, 'eval_steps_per_second': 8.056, 'epoch': 1.0}\n",
      "{'loss': 0.0236, 'grad_norm': 0.9697262048721313, 'learning_rate': 4.027557612291482e-05, 'epoch': 1.1}\n",
      "{'loss': 0.019, 'grad_norm': 1.1644240617752075, 'learning_rate': 2.531716231303287e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0194, 'grad_norm': 0.7755067944526672, 'learning_rate': 1.2483590795369743e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0199, 'grad_norm': 0.9264175891876221, 'learning_rate': 3.6320853642843657e-06, 'epoch': 1.75}\n",
      "{'loss': 0.0201, 'grad_norm': 0.7252911329269409, 'learning_rate': 4.3600933994007996e-08, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9996892ea62745ac8711bbc016ad889d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03134845942258835, 'eval_pearson': 0.7445355318621882, 'eval_runtime': 44.2218, 'eval_samples_per_second': 64.29, 'eval_steps_per_second': 8.05, 'epoch': 2.0}\n",
      "{'train_runtime': 3792.4665, 'train_samples_per_second': 19.234, 'train_steps_per_second': 1.202, 'train_loss': 0.024841662823108204, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(); #run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = \"albert-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm) #our special tokenazier for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁i', '▁think', '▁this', '▁would', '▁be', '▁the', '▁last', '▁one', '!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz.tokenize(\"I think this would be the last one!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ec101d59244c189a87d4a9df821f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789796ddf22140aa90773b18383bbb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47997a04b8d14c15ab8eabb8a3f65a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tok_func(x): #a function that back tokenizied inpots\n",
    "    return tokz(x[\"input\"])\n",
    "tok_ds1 = ds1.map(tok_func, batched=True)# a mappinfg for doing parallel running that adds a new item to our dataset called input_ids\n",
    "tok_ds2 = ds2.map(tok_func, batched=True)\n",
    "tok_ds3 = ds3.map(tok_func, batched=True)\n",
    "tok_ds1 = tok_ds1.rename_columns({'score':'labels'})\n",
    "tok_ds2 = tok_ds2.rename_columns({'score':'labels'})\n",
    "tok_ds3 = tok_ds3.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def corr(x, y):\n",
    "    return np.corrcoef(x, y)[0][1]\n",
    "\n",
    "def corr_d(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Flatten the predictions if needed\n",
    "    if predictions.ndim > 1 and predictions.shape[1] == 1:\n",
    "        predictions = predictions.squeeze(axis=1)\n",
    "\n",
    "    return {'pearson': corr(predictions, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "bs = 16 #our batch size\n",
    "epochs = 8 # and number of epochs \n",
    "lr = 8e-5 # starting learning rate\n",
    "args = TrainingArguments('outputs',#the directory where our wheight will save\n",
    "                        learning_rate=lr,#specifieng learning rate for start\n",
    "                        warmup_ratio=0.1,#how to increase learning with steps\n",
    "                        lr_scheduler_type='cosine', #lower the learning rate using a cosine curve \n",
    "                        fp16=True,\n",
    "                        load_best_model_at_end=True,\n",
    "                        evaluation_strategy=\"epoch\", #evaluate the model after each epochs \n",
    "                        save_strategy=\"epoch\",        \n",
    "                        metric_for_best_model=\"pearson\",  \n",
    "                        per_device_train_batch_size=bs, #size of our batch \n",
    "                        per_device_eval_batch_size=bs*2, \n",
    "                        gradient_accumulation_steps=2,  \n",
    "                        num_train_epochs=epochs,#total number of training epochs \n",
    "                        weight_decay=0.01,#usinh L2 regularzation\n",
    "                        report_to='none')# our argument for fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) #create our model\n",
    "trainer = Trainer(model, #specifieng model\n",
    "                args, #illustarate arguments\n",
    "                train_dataset=tok_ds1, #our train set\n",
    "                eval_dataset=tok_ds2, #our test set\n",
    "                tokenizer=tokz, #out tokenizer\n",
    "                compute_metrics=corr_d, #what is our metrics\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stops after 2 evals without improvement\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f84a92bd50e461c847c558a3e6424c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0057, 'grad_norm': 0.3648189604282379, 'learning_rate': 4.385964912280702e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0117, 'grad_norm': 0.8302813768386841, 'learning_rate': 7.997782552120674e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ac25f4e9784c1d87f559d21001772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03788059949874878, 'eval_pearson': 0.7015678476159913, 'eval_runtime': 42.6373, 'eval_samples_per_second': 66.679, 'eval_steps_per_second': 2.087, 'epoch': 1.0}\n",
      "{'loss': 0.0137, 'grad_norm': 0.7276374697685242, 'learning_rate': 7.899468161955669e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0134, 'grad_norm': 0.6421157121658325, 'learning_rate': 7.658775221045979e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09459ba88f1847198b181fea60f82ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03828788176178932, 'eval_pearson': 0.7026296752754126, 'eval_runtime': 42.5978, 'eval_samples_per_second': 66.741, 'eval_steps_per_second': 2.089, 'epoch': 2.0}\n",
      "{'loss': 0.0116, 'grad_norm': 0.4416433870792389, 'learning_rate': 7.285365556448004e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0096, 'grad_norm': 0.5355353355407715, 'learning_rate': 6.791381161493047e-05, 'epoch': 2.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5041e4f4023041db911646f826357081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03270522132515907, 'eval_pearson': 0.7234392620530207, 'eval_runtime': 42.623, 'eval_samples_per_second': 66.701, 'eval_steps_per_second': 2.088, 'epoch': 3.0}\n",
      "{'loss': 0.0096, 'grad_norm': 0.352676659822464, 'learning_rate': 6.195477020602767e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0069, 'grad_norm': 0.3146379888057709, 'learning_rate': 5.519410964445069e-05, 'epoch': 3.51}\n",
      "{'loss': 0.0073, 'grad_norm': 0.6050491333007812, 'learning_rate': 4.787867719573825e-05, 'epoch': 3.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112057bca6be47e4a0e9e08bf92e85ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.032055843621492386, 'eval_pearson': 0.7331759243321564, 'eval_runtime': 42.6051, 'eval_samples_per_second': 66.729, 'eval_steps_per_second': 2.089, 'epoch': 4.0}\n",
      "{'loss': 0.0051, 'grad_norm': 0.42970868945121765, 'learning_rate': 4.027557612291482e-05, 'epoch': 4.39}\n",
      "{'loss': 0.005, 'grad_norm': 0.24761943519115448, 'learning_rate': 3.266241313206286e-05, 'epoch': 4.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282d6899e10b4f46b1b124dc7f28dcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.031195566058158875, 'eval_pearson': 0.736784596561627, 'eval_runtime': 42.6241, 'eval_samples_per_second': 66.699, 'eval_steps_per_second': 2.088, 'epoch': 5.0}\n",
      "{'loss': 0.0037, 'grad_norm': 0.26316943764686584, 'learning_rate': 2.531716231303287e-05, 'epoch': 5.26}\n",
      "{'loss': 0.0032, 'grad_norm': 0.07472123205661774, 'learning_rate': 1.8508015666127043e-05, 'epoch': 5.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b787b01af604de49ed94f0ecb5c8580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03233282268047333, 'eval_pearson': 0.7282306199217221, 'eval_runtime': 42.6147, 'eval_samples_per_second': 66.714, 'eval_steps_per_second': 2.088, 'epoch': 6.0}\n",
      "{'loss': 0.003, 'grad_norm': 0.10132621973752975, 'learning_rate': 1.2483590795369743e-05, 'epoch': 6.14}\n",
      "{'loss': 0.002, 'grad_norm': 0.20898258686065674, 'learning_rate': 7.463853308003739e-06, 'epoch': 6.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20674b69ab74841a1495d75f0bec47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03227576985955238, 'eval_pearson': 0.7320897854417371, 'eval_runtime': 42.6604, 'eval_samples_per_second': 66.643, 'eval_steps_per_second': 2.086, 'epoch': 7.0}\n",
      "{'train_runtime': 12704.5515, 'train_samples_per_second': 22.967, 'train_steps_per_second': 0.718, 'train_loss': 0.007115774704400161, 'epoch': 7.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(); #run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"albert-base-v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
